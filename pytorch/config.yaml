wandb:
  project: "202602_smc"
  name: "${feature.audio_feature}-${model.type}-${score_informed.method}-${loss.loss_type}-k${loss.kim_loss_alpha}-l1${loss.w_l1}-b${loss.w_bce}-d${loss.w_delta}-uh${loss.use_huber}-wh${loss.w_huber}-hd${loss.huber_delta}-ext${model.input2}${model.input3}-sr${feature.sample_rate}" # -iter${exp.total_iteration}
  comment: "mel128"


exp:
  workspace: "./workspaces" #kaya
  # workspace: "/media/datadisk/home/22828187/zhanh/202510_hpt_data/workspaces" #uwa
  # workspace: "/Users/hanyu/Documents/ipynb/202510_hpt_data/workspaces" #mac
  # workspace: "/media/mengh/SharedData/zhanh/202510_hpt_data/workspaces" #mengh
  total_iteration: 120_000  # 200_000 # 300_000 # 360_000
  save_iteration: 10_000    # 6_000 # 6_000
  eval_iteration: 10_000    # 6_000 # 6_000

  optim: "adam"             # "adam" | "adamw" | "ranger"
  learnrate: 1e-4           # 3e-4 # 1e-4
  weight_decay: 0.0
  decay: True # False
  reduce_iteration: 10_000 # 6_000
  random_seed: 86

  mini_data: False # True when developing model
  cuda: True
  num_workers: 12 # 6 for Score Calculating
  batch_size: 12

  # Infer and Eval
  run_infer: 'single' # 'multi'
  ckpt_iteration: ""  # 180_000
  ckpt_file: ""

optim:
  name: "adam"          # "adam" | "adamw"
  lr: ${exp.learning_rate}
  weight_decay: 0.0


feature:
  audio_feature: "logmel"
  classes_num: 88
  # ---------------------------
  # dataloader.py params
  segment_seconds: 10.
  hop_seconds: 1.
  # ---------------------------
  # feature_extractor.py params
  # mel_bins: 229 # 去feature extractor里面手动定义吧
  sample_rate: 22050
  fft_size: 2048
  frames_per_second: 100
  augmentor: null
  # ---------------------------
  begin_note: 21
  velocity_scale: 128
  max_note_shift: 0


dataset:
  # Fix "maestro" for (train valid), select "smd|maestro|map" for (test)
  train_set: "maestro"
  test_set: "smd"

  # Use the same workspace root as features.py packs to
  train_h5: "${exp.workspace}/hdf5s/${dataset.train_set}"
  test_h5: "${exp.workspace}/hdf5s/${dataset.test_set}"

  # Provide dataset dir to process the features.py
  smd_dir: "../Dataset/SMD"
  maestro_dir: "../Dataset/maestro-v3.0.0"
  maps_dir: "../Dataset/MAPS"


# ---------------------------
# AMT modules (base model + adapter + conditioning)
# ---------------------------
model:
  type: "hpt"  # hpt | hppnet | dynest
  params: {}
  # score-inf condition selector:
  # direct_output: ignore input2/input3.
  # bilstm/scrr/dual_gated: cond = selected non-null inputs from input2/input3.
  # note_editor: input2 must be onset; input3 is null/frame/exframe.
  input2: null # onset | frame | exframe | null
  input3: null # onset | frame | exframe | null
  kim_condition: "frame" # for FiLMUNetPretrained only: frame | none
  pretrained_checkpoint: "/media/datadisk/home/22828187/zhanh/202510_hpt_data/workspaces/checkpoints/FiLMUNetPretrained+frame/1000000_iterations.pth"
  hppnet_model_size: 128


score_informed:
  method: "direct_output"     # direct_output | scrr | dual_gated | note_editor | bilstm
  params: {}
  train_mode: "joint"         # joint | adapter_then_score | adapter_then_joint
  switch_iteration: 100_000    # phase switch point for staged modes


post:
  post_processor_type: 'regression' # 'onsets_frames'
  # regression (0.1, 0.3, 0.3), onsets_frames (0.5, 0.1, 0.3)
  frame_threshold: 0.3 # 0.1 in RegressionPostProcessor, 0.3 in score_calculator
  onset_threshold: 0.3
  offset_threshold: 0.3
  pedal_offset_threshold: 0.2

# ---------------------------
# Loss configuration
# ---------------------------
loss:
  loss_type: "velocity_bce"
  kim_loss_alpha: 0.5  # Weight of BCE term in Kim et al. 2024 BCE+L1 loss
  w_l1: 1.0
  w_bce: 0.5
  w_delta: 0.01  # set 0 to disable delta regularization
  use_huber: true
  w_huber: 1.0
  huber_delta: 0.1
